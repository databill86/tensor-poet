{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TextLibrary class: text library for training, encoding, batch generation,\n",
    "# and formatted source display\n",
    "class TextLibrary:\n",
    "    def __init__(self, filenames):\n",
    "        self.filenames = filenames\n",
    "        self.data=''\n",
    "        self.files=[]\n",
    "        index = 1\n",
    "        for filename in filenames:\n",
    "            fd={}\n",
    "            fd[\"name\"] = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.c2i = {}\n",
    "            self.i2c = {}\n",
    "            try:\n",
    "                f = open(filename)\n",
    "                dat = f.read()\n",
    "                self.data += dat\n",
    "                fd[\"data\"] = dat\n",
    "                fd[\"index\"] = index\n",
    "                index += 1\n",
    "                self.files.append(fd)\n",
    "                f.close()\n",
    "            except OSError:\n",
    "                print(\"  ERROR: Cannot read: \", filename, e)\n",
    "        cs = set(self.data)\n",
    "        csi = list(enumerate(cs))\n",
    "        self.c2i = {c: i for i, c in csi}\n",
    "        # csi = list(enumerate(cs))\n",
    "        self.i2c = {i: c for i, c in csi}\n",
    "        self.ptr = 0\n",
    "            \n",
    "    def printColoredIPython(self, textlist, pre='', post=''):\n",
    "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n','<br>')\n",
    "            if ind==0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind%16]+\";\\\">\" + txt +\\\n",
    "                       \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "        \n",
    "    def sourceHighlight(self, txt, minQuoteSize=10):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc=[(\"Sources: \", 0)]\n",
    "        sc=False\n",
    "        noquote = ''\n",
    "        while len(tx)>0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p<=len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p<=len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1>mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f[\"name\"]\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote)>0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ],mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN,mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote)>0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.printColoredIPython(out)\n",
    "        if len(qts)>0:  # print references, if there is at least one source\n",
    "            self.printColoredIPython(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "    \n",
    "    def getSlice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "    \n",
    "    def decode(self, ar):\n",
    "         return ''.join([self.i2c[ic] for ic in ar])\n",
    "            \n",
    "    def getRandomSlice(self, length):\n",
    "        p = random.randrange(0,len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "    \n",
    "    def getSliceArray(self, length):\n",
    "        ar = np.array([c for c in self.getSlice(length)[0]])\n",
    "        return ar\n",
    "        \n",
    "    def getSample(self, length):\n",
    "        s, rst = self.getSlice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "    \n",
    "    def getRandomSample(self, length):\n",
    "        s = self.getRandomSlice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "    \n",
    "    def getSampleBatch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.getSample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "        \n",
    "    def getRandomSampleBatch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.getRandomSample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The tensorflow model for text generation\n",
    "class TensorPoetModel:\n",
    "    def __init__(self, params):\n",
    "        self.vocab_size = params[\"vocab_size\"]\n",
    "        self.neurons = params[\"neurons\"]\n",
    "        self.layers = params[\"layers\"]\n",
    "        self.learning_rate = params[\"learning_rate\"]\n",
    "        self.steps = params[\"steps\"]\n",
    "        # self.clip = -1.0 * params[\"clip\"]\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Training & Generating:\n",
    "        self.X = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
    "        self.y = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
    "\n",
    "        onehot_X = tf.one_hot(self.X, self.vocab_size)\n",
    "        onehot_y = tf.one_hot(self.y, self.vocab_size)\n",
    "\n",
    "        basic_cell = tf.contrib.rnn.BasicLSTMCell(self.neurons)\n",
    "        stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * self.layers)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32)\n",
    "        self.init_state_0 = stacked_cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "        self.init_state = self.init_state_0\n",
    "\n",
    "        with tf.variable_scope('rnn') as scope:\n",
    "            rnn_outputs, states = tf.nn.dynamic_rnn(stacked_cell, onehot_X, \n",
    "                                                    initial_state=self.init_state, \n",
    "                                                    dtype=tf.float32)\n",
    "            self.init_state = states\n",
    "\n",
    "        self.final_state = self.init_state\n",
    "        stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, self.neurons])\n",
    "\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [self.neurons, self.vocab_size], \n",
    "                                    initializer=tf.random_normal_initializer(), dtype=tf.float32)\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [self.vocab_size], dtype=tf.float32)\n",
    "            \n",
    "        logits_raw = tf.matmul(stacked_rnn_outputs, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits_raw, [-1, self.steps, self.vocab_size])\n",
    "\n",
    "        output_softmax = tf.nn.softmax(logits)\n",
    "\n",
    "        self.temperature = tf.placeholder(tf.float32)\n",
    "        self.output_softmax_temp = tf.nn.softmax(tf.div(logits, self.temperature))\n",
    "\n",
    "        softmax_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=onehot_y, logits=logits)\n",
    "\n",
    "        self.cross_entropy = tf.reduce_mean(softmax_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "        self.training_op = optimizer.minimize(self.cross_entropy)\n",
    "        \n",
    "        # Clipping isn't necessary, even for really deep networks:\n",
    "        # grads = optimizer.compute_gradients(self.cross_entropy)\n",
    "        # minclip = -1.0 * self.clip\n",
    "        # capped_grads = [(tf.clip_by_value(grad, minclip, self.clip), var) for grad, var in grads]\n",
    "        # self.training_op = optimizer.apply_gradients(capped_grads)\n",
    "\n",
    "        self.prediction = tf.cast(tf.argmax(output_softmax, -1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.y, self.prediction)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        error = 1.0 - self.accuracy\n",
    "\n",
    "        \n",
    "        # Tensorboard\n",
    "        tf.summary.scalar(\"cross-entropy\", self.cross_entropy)\n",
    "        tf.summary.scalar(\"error\", error)\n",
    "        self.summary_merged = tf.summary.merge_all()\n",
    "\n",
    "        # Init\n",
    "        self.init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textlib = TextLibrary([  # add additional texts, to train concurrently on multiple txts:\n",
    "                       'data/tiny-shakespeare.txt',\n",
    "                       # 'bk/emma-jane-austen.txt',\n",
    "                       # 'bk/voyage-out-virginia-woolf.txt',\n",
    "                       # 'bk/pride-prejudice-jane-austen.txt',\n",
    "                       # 'bk/wuthering-heights-emily-bronte.txt',            \n",
    "                      ])\n",
    "\n",
    "params = {\n",
    "    \"vocab_size\": len(textlib.i2c),\n",
    "    \"neurons\": 64,\n",
    "    \"layers\": 2,\n",
    "    \"learning_rate\": 1.e-3,\n",
    "    \"steps\": 32,\n",
    "}\n",
    "\n",
    "model = TensorPoetModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training:\n",
    "max_iter = 1000000\n",
    "batch_size = 32\n",
    "generated_text_size = 500\n",
    "epl = len(textlib.data) / (batch_size * model.steps)\n",
    "restoreCheckpoints = True\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model.init.run()\n",
    "\n",
    "    tflogdir = os.path.realpath('tensorlog')\n",
    "    if not os.path.exists(tflogdir):\n",
    "        os.makedirs(tflogdir)\n",
    "        print(\"Tensorboard: 'tensorboard --logdir {}'\".format(tflogdir))\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(tflogdir, sess.graph)\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    \n",
    "    # Used for saving the training parameters periodically\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint_file = os.path.join(tflogdir, 'model.ckpt')\n",
    "\n",
    "    start_iter = 0\n",
    "    if restoreCheckpoints:\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter=int(lastSave[pt:])\n",
    "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "    \n",
    "    for iteration in range(start_iter, max_iter):\n",
    "        # Train with batches from the text library:\n",
    "        X_batch, y_batch = textlib.getRandomSampleBatch(batch_size, model.steps)\n",
    "        i_state = sess.run([model.init_state_0], feed_dict={model.batch_size: batch_size})\n",
    "        i_state, _ = sess.run([model.final_state, model.training_op],\n",
    "                              feed_dict={model.X: X_batch, model.y: y_batch,\n",
    "                                         model.batch_size: batch_size, model.init_state: i_state})\n",
    "\n",
    "        # Output training statistics every 100 iterations:\n",
    "        if iteration % 200 == 0:\n",
    "            ce, accuracy, prediction, summary = sess.run([model.cross_entropy,\n",
    "                                                          model.accuracy, model.prediction,\n",
    "                                                          model.summary_merged],\n",
    "                                             feed_dict={model.X: X_batch, model.y: y_batch,\n",
    "                                                        model.batch_size: batch_size})\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            ep = iteration / epl\n",
    "            print(\"Epoch: {0:.2f}, iter: {1:d}, cross-entropy: {2:.3f}, accuracy: {3:.5f}\".format(ep, iteration, ce, accuracy))\n",
    "            for ind in range(1): # model.batch_size):\n",
    "                ys = textlib.decode(y_batch[ind]).replace('\\n', ' | ')\n",
    "                yps = textlib.decode(prediction[ind]).replace('\\n', ' | ')\n",
    "                print(\"   y:\", ys)\n",
    "                print(\"  yp:\", yps)\n",
    "\n",
    "        # Generate sample texts for different temperature every 500 iterations:\n",
    "        if (iteration+1) % 500 == 0:\n",
    "            \n",
    "            # Save training data\n",
    "            saver.save(sess, checkpoint_file, global_step=iteration+1)\n",
    "\n",
    "            # Generate sample\n",
    "            for t in range(2, 11, 4):\n",
    "                temp = float(t) / 10.0;\n",
    "                g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "                xs = ' ' * model.steps\n",
    "                xso = ''\n",
    "                for i in range(generated_text_size):\n",
    "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp], \n",
    "                                              feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                         model.batch_size: 1, model.temperature: temp})\n",
    "                    inds=list(range(model.vocab_size))\n",
    "                    ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                    nc = textlib.i2c[ind]\n",
    "                    xso += nc\n",
    "                    xs = xs[1:]+nc\n",
    "                             \n",
    "                print(\"----------------- temperature =\", temp, \"----------------------\")\n",
    "                # print(xso)\n",
    "                textlib.sourceHighlight(xso, 20)   # 20: minimum quote size detected.\n",
    "            print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating text using the model data generated during training.\n",
    "def ghostWriter(textsize, temperature=1.0):\n",
    "    xso = None\n",
    "    with tf.Session() as sess:\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath('tensorlog')\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(tflogdir))\n",
    "            return None\n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, 'model.ckpt')\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter=int(lastSave[pt:])\n",
    "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
    "            return None\n",
    "\n",
    "        g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "        xs = ' ' * model.steps\n",
    "        xso = ''\n",
    "        for i in range(textsize):\n",
    "            X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "            g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp], \n",
    "                                      feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                 model.batch_size: 1, model.temperature: temperature})\n",
    "            inds=list(range(model.vocab_size))\n",
    "            ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "            nc = textlib.i2c[ind]\n",
    "            xso += nc\n",
    "            xs = xs[1:]+nc\n",
    "    return(xso)\n",
    "\n",
    "\n",
    "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.sourceHighlight(generatedtext, minQuoteLength)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring checkpoint at 4500: /home/dsc/git/AI/tensor-poet/tensorlog/model.ckpt-4500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "lyaBlu$eOPe Y!'e-u..e$ leNOa$e;yOrXX?OK.'e-OO.h'e$ OKe Y:'lyleK:'O-se.OKNaalu::lqXXCamGjUmHXjO-$uyNl.YyqXXdO-en3ttAteWIAB3HXvlPeK:ha$HXmK$ePu$ u:eLOKe$ Y$eOyle-lyleYTle!LqXXiB3tUbmHXdlaweP.Y$l'h'e-ul'eYe Y$ O:we;Y$ u:aHe$ lflecO!le lleY.YLe Ya$eaOe$Oe;le$yO-qXXol..O-:we$Oe.uyae!lyle-u$$eUe!YaeY:'alysXBYNle!Ka$eOKye lePOK.$eYyTe!K$eafYLF;ylePyucfylY$HeU$e:O:e YP&eUe:Y'e3K!e$OK.aeY..e$ Y$e Yae!Lah'ePOyeaY:Te$ Y:leK.O':ecO!YTlrXdY$ e-uy'eLOKeu'eal..e-le.Y'ulyHXD OOe.lf.'h'eLOKyea lYy'weLu$e'&eA- Y..ea$OKNlwe-u$:'wXpO-uO:qXXvKDUeUe:OTe$Oea$e.uc$uOalXD lfae;Oe OO'e?Y;lel:Yal'e- uc eU:YaeLOK..laePK.'e$ llyeu:e$ uae-ua$eyu$.ul:$e u'ea lYye Y'clwXUe!le;K$e lyeZYKyfhale O.'evYcTYae!Le'..l'e$ OK..qXXWG?3tUDHXnO!e;lweT:O-Te- Y$eEl:eNKcYa Y..e$Oe!leT:O:e u!qXXtm?3ttoUmHXjY$O$eau:cl'LweLOKeaO$:u:NaeYecyK:Nae$ Y$.Lel:$lse$yOe$OO-HXD l$Y.&e;K$eYae.llPKLe$Oe!u.Le$ Le;lFu:Na$e!Y$lyl:eY..HXRO-.e-lwe$uf.larXX?3WUtdA?bUHXdO-e!Lel.Yu:l:XD l:wXdlredO-aecyla$u:NkNO$e YHedlzfl:$'Ll'ePY$ lyclefO!YLe- lLe- ..l$eiu$ l:eOPe- lyePl..XjO!la$e laXtOyalqXtOcae$ uae;KNl.qXXR QBAtHe3WYFle-lyaej.la$OKa$e leOPyl.'l'eale$yl;Ku:NeO:e!LweaKc $uy'He.u$ PKlyeKaeY$ lle u!leO:eY-eaOe ly!edOe!Lea$O.'weYyyaeLKaePOylaeO:lefO:lHXR yLe$ Le.O!lyareZle YFLeale!Y$we!Le uaweYy$ule'Oe:ua e'lYyLHXU:lePllywe$ Y$ecOyfPuPuae$ Le.O!l'eall!aalweLOKye!u:Ne$Oe.O:NhPeLOK.'a-uyNY'eOKN:l:qeR Y$ hea Y$e'O$eu:e$ leFl:$!ly$e-Y.'e$ l:e$ O-we'lKl$ealyla$ea$YuawXD Y$e?YylY!e$ lea-u!ul'aeLOKe:OO$$Y:se$ leLOKe-lLwXRu..e$ Kle;yYT$e:Oe;lae;yOO:lweoK$e YFlae.OafOLqXXdAWn3ed3WHXUa$e!Le.OT:e Oal'e;O$u..HXR $cOKyamwXUePylY$&e;LeMOPefyOLOHeUe YFlePOeTlylaqXXCGbUteoAAWmHXdll:alye!Yy'e$Oe!lY$euaePY;.Yu:leaY;l'ePO:Te YaqXXmimHXdl:'ecO!;Le$ uN :$Hea Y$e:O;yKae!u..wXUecO'ePOy$ea leDOeNO-u..e!LePOye-u..e;Leau:$lXUh$.Y!Xou$wel:alLe'lefY:'e.ll:'lyauOwe;Yclwah'ea luaeWuNalHXDOe- uN $we Y-eDule-ua eK'eFY:'l:'HXDOea fll$h'lwXD OKN $eLOKe!OylhaeY:'HXAae.YLeNyYuylhHXD lc e!Oel:O-Hebl.Ye :O-lrXXn3tmQeAtmHXSYPe.'eY:c.Oa$e-l.$l'sXU:e-l..ePO:eK:Xdle'u!lleh$uawe YaePyOPe!Kc Yu:'ea uc e!Oy'weY:'e$Oe;Ou:NasXmFl:NeOPeduN $wXDyY:eY''XUe Y.u:Ne$Oea Y:'e;lyO:lwXvK$yeY!lweY:'el:$OKNTaeEu''l:wXtuy'ualOPrXQOye$ ucl'e$ OKye!lYfulcOe:OK.$O-e;ua$eaaeY.TweUaeY:'e.Oy'we-lLeNYTle!K:Ne'lY:uaNe$ leT:O.'eaO-eau$XUe3YFlzXtOu.eYeNOO::uylaeYy'e:OO$ lle$ le$ le-lae YFle;ylFly$eLOK:haePyu$ K$e$ leWUeiY:'weY:.Y$ l:Xv:cO:ea$yu:aweUe.OFlaeaY;cu:$O:kal:'se$ Y$e;Le- leau$ HeLOK.eLOKyNe$Oe:Oa$eKaTe$ lePO:clweaK..OKyawXRuN:lYyle:O$e- Le..u$lweyO-e!Ka&XdO..e lY'eOPeKOyeLOKye!uc eLOKe;Kc e.uNu' rXXvWiDUtUjHXR Y$ePlyVYyule :O!yLsXX$BO$ Yyle Y$e$ lye$ Y$ea l:ea OKXR YLeNOy'HXAeLOKe'ylalcO:'e:l:e lfflyeY-a$ XD lYylwejO$lXD Y$eY:'KVKl:XvKaea uawea uae'lYFlysXdOeaY-YylwXmwe:O$e- uc e'leA:'e$Oe'ul:$wXQu..aYh'e!LeYeu$eu$eYae-uc qXXR3WdQZmHXRu$ eauae Yffl.qXXp?3n3HXbl'egO'l:awX?Y:'eY:'eflYLlweVKa$e;lYyclYLePl$leyl:a $l.qe$ Le$ Y$eY:'e;Le;yl$Oe;lFlyl:ku.weUeOPeTl:Nlye!K!uawseOPe-u$ e YFl'eh$ Y:$kLOeale!YTlyeauO.HeUeafY:'e-O..e:O'wXALqXoO$OaXGau..ecAeNK$uPlXA:'eYe'O:l:$!l:harXoO-aeal.LeaPOK.'h'eau:$HXRu:Nlae!l:Y$u'lwe;le!uyNlX'l.uO!wXD l:eUeW.Y!eLOKE&XXnlYFu:OhePOye!Yu:Ne OFleEu$YhlHXD Y$.u;Puae'O:aLeNOe:le!LarXXn3WZmbj3tHXD OK.weh$Y.e l!Y:e$ Y$O:lefll:.Oyae:O!l$Xm:eY$ e$ lLeRO!u.uO:e-uY:weOPeY:'OyXju$ weYaTe.OTlHXAaeaKLe$ YKaKe-u..h'eL-KulecyOu:'elYFle$llyweOPe!KawXR yleuaePu$ u:la$&eYae;:O$ lyh'eoY:'Llaell'haePYua$e leiuyeLOK.F-QOy'e$ Y$ul:$lyaweuTe:OLe Y$ e:u$ e'Y$le'lafuly!O:OweNyula$e$ ylwXtOe.OOKa$eLOKecu..ea$Y:lae'Y!lac eNO'aqeYLlejOK.$uO.HXvK$eUe.O!rXvOe$ Y:Ney.K$.wXpOhah.ual.Ya$O-LeOPe:OeOKyeu:eLOKXjO-eUefYyfO:we lylaalyu:'wXA:'e:Yae$O.leTu:$O- eUe!YLlqXouPP.uu:haeu:eY:clwe$ l$eYclefyllyKa.lse$ u:eT:OK:u:eOflu$ e'lLeY:'eY:e Y$eUeYPeY:fOy'e$OeNOPcOKyK'evLOKye'llYy'e!leOPeFu:Nul:we!LeYPyLKaeYecYKflXDOe;Ka$e;O$e;lYy$LeOPe.O'wXR lyeau:NuOP$eLOKePOyle$ OKe!Y:lwXQO!le$u!wePOu!l$ u:u$le$ Y$eu$ePu$e$ leNyl;u$ul:rXXCOy e!YFlqXXpbmGDUoHXOKeuPe.Yfe$ uae YFle:alYyTlXole-Oe!Y:'e?Kye!OKye$KeTOElrXXQUtBAeWmtDmHXUPh'ec u:NseOKye$Oe!l:$&XQOLeWKl'a lyweLmKalye-fO:Oe$ leiO!'e-u$ u:'rXXblPu$Ohh.HXRu$ e.Yy'alXnOPu$ e!Oa$Xj YLe.le!Ya$:lseYawe YFleOPeYFl:HXD lyleLOKe lye?Ya ly$e:O$e$ OKe!YL&XD Y$eOPenKuT:sXQyYa$.l'e Y$aeVK:O-HXD OKecOK:'$Le YFle lyle:Ol'eOK:$lyrXm-e$uOqe3D.Y:'eUPeY.'efO:'Kyu:'Y:Lwe-OK:$e$ Y$eUha$e;le$ uae:O-eY:eNO-we;KNe-Oe..&XLOKyeY:'e$ Oe;lOTu:NXvl$e llyclae: uaeNY;.u'ul:e:OealLeyKY$euaeLOKe$Oe!u:we u'eyO-:Oye:O-e uawe;l$e$K.lea le!Y$ hle- lY'$yu:NeY:'e$ YKTe;OLea-l$euaeKK$ eY:'we!Le.lY'HeVK;.leNyu$ lyaXm:eO:'e!lfu.leY'sXnOK.'ea$llye$YLeYe;lcLePK.O$e$Y;lu:layY'qeUWlae:O-e- YLweaUec.Y:u:NwXbllaK.leuae<br> uc Yy'XBLe-Oe Y$ ea$ lye uaeu:e$ Y$eUe;la$eY:'eaOKy;l:$XmPe!YLXnyO$e!LeO:ecO$ eLOK:'a$LePY.lePYyleFu$uOPeY:'eY'leOKyXR Y$e-lYcuO:$Le:O$e'Ya u:NePOyeY$ePOOe O!Y:e'Oe$ Y$e$ Y$e!LweDYc weYaeY:'YLeful:cOK$u:kcdY:'rXW Le lyle.O!Y:&eY:'e;Le;KN Y:cO-wXvPylY$ul:wePOye'u$ lyh'eY:'e!le lleOPO-XUeYe$ lePO$.YLeUeYFuyh'e:OO!le-u$e uae:O-:OyweTl:alea$OO:&ePmHecOe;yO$el.$Kl:NeY$HXR yuNKNlqXXCbumweUAoHXdlec Yu.eLOK$XDOefu$ul.ae$OO.Ku:leLOKe Y..LHXUPe u!e$ Y$l!lyleu:'yaua$KaXDOeNu..eafYu.weY:'eYePOyyLreolyu$ uOPe u:lwXU:'e'OeaOyleLOKea$e- lywe lyal..ePOya$e;Yy$weVKauNcY'wX$ le$-Ye$Oe.la$e.OTleYe$ OKe Yy.haeNOO'euPeuPef.Y:'lye:O-e$ Y$$qXXBSU3WPeUmtye$OOK'k:O$:ly.e!le Y-eKfTLe'Y.YFlweaOwe-OK.a&XdOK.PK..qXvK$e!Y..rXXdGj3tUHXme'ua$sXm:YaweY:'e$Oeazl-lyle OKaecO!X$ Y$$wXmPe.Y!euPe;.Y:THXnK$ePyuN erXWylY.u'lqeYLeYae-Oy.'e;K$e$ lePOyc u:NK:e l!LeLOPe!le:u'e?OKye$ le.ulNleO:seLK$e.Y:$O:XdlYyqXXCGj3tUmHXD lye lyO!lweLOKyeO'eLwe'YYa$e$ lealy!O:Tqe3W$e uae-Ya$lclqXDOe!l:sXD lweu:'XZlFluO:$Le$ l:ecOK.'ePOylqeuae:OeNyOFle;ylY$$Y:cO:eNlu$weY:Ne;.Yu$l'Xdl;ylYFleNu$$weNOOarHXR l$e uae-u'ae!lwe.Y$lye.la$lyl:NXD le!la$eyuFlaweYuawe-l$e'OKcl$e Ye.K'la$seY:'eLsXY:'e-Ye;yual:weLOkk;Y:$e lyeLle.YL$K:$lau:lwe-OeNlcle.OFlqXXAb3bHXBYy'll.we$ le Y'e$ Le;Ya$&epYe YFle$ le lYye'OaNK:$ eOPe uFl.lea$YyleOKye uaeV:OKN e!OOyawe;lVKY.;Xpul:$lXAep$;OLe.ulyqXdlYyu:e OKeT:Oye$ l:lyqXXdUGoo3bHXbOKyeKY$ule;l'e- l.$ea$e!Y$lyKaeh$yeuPe uKaaeyuTl'u.ae!l:$leu:e$ Y$eLOKe'Ka$u!LweAaalyl'rXD Y$eUeaY'e;LeKPPlc.u:NO$wea Y..e!Le:l;yle$u!awXA:'eNyu'Ne'Yu$lqXA:'wXA:'e$ Y$e'KuN uae fyl$ly$ea ue:u-KaOK:'HeWle-lyl'auFu:$eLOK:e$ Ku:ea-OyTl.$ae!l$lyuNYa'LXmPe lyawe leO!fuyae$ u:l:awe'll$we-u$ ePY$ eLOKe Y..e!LePu..elO:NYcleY:'eY:'e!Y'u$ul:weu:e$ QCu!e!YLwePuPe$Oe;yO$l:haeYae;KcleUePOy$ e.l$e$ le.O$ eOPh.u:lefYLeYy$ lye;OTl'weu$e:O$e$ le!YLePyYa$lh$X-u$ e' OO:lwXdYu$ Lweal..e'Oe!LOPeLwe-l..e u'e$ OKe!O-ea Y..ePl.lwX- leilyOyl'qeiuy'eou$KePu..lahwe'O$$e$ Y$eY:!e.OTuNl:haeY:'aleylcThl:aef.uO!e$ OK:eO;yePOO'eOPeOPe$ l!efylFuN O:lsXU:le$ O!eYPO!LOKe$ le!LkOPefyO$ LleY:$ePyO!laeLOKellyOKy$efYLLweUh..edlule$ $eOKXQOye YFl'eO-e$OecOTlXnyla$Ky$Klae$Oe;ylweTul'e ucla$e Y!eOPe$ uae-lc e:O;.Yf$Le'OylHXD lrXXbvUtUtb3oHXRl$e$ lelKfae$ Le!Le!lTqXXD?UADW-HXR -uacuclePYyl$we- lylse$ OK:leYy'eu$eOPPyu'lawXfyYf.u:Ne uae$ Y$e.u$e!laO:wXA:'e-laclwXA:'e$ LeuKaLqXXWCGjUmHXoyLle lfeOK!KaKleuaelNu.u'e'O!l!e!u:laweY:'qXXJbmRp?mtBHXme3tWUHXvl$e$ OKe Y..O:we OLeYePLK.e OOye'uau:cu:lh'XR ly'eUeyY:NlyOK$leY-ec uaeYFY:eYyel3ae$ O-eLOKel:'u:Neu:e$ le$ Ouaea$LqXXAGd3e3iUZHXR Y$eTu:Nhh'eyYa$ae Y..Y$&XR Y$eUe;lPyLeolXR Oae;lh!l'XAf.O:Nae!Y$l. e$ Y$e u!we- le'lle-lu.eLOKy&XXWGjdAWDHXD Y$eLOKea Y$fLqea Y:eNyO$Ksea O!e!u'XRYLeu:e Y:'e!Oe O! e!O:cu:Ne!uN $l:qXXCG?UDAHXR O$eaOa$e:Y$:uaclwePKyY$ae$ u e:O$eLOK$HeNyO'eLOKPOy'.Le u.'eLOKe$Oe;yYLeau.e YFle u!e!Ye$ Y$e'u!le!Leau:N.ua$e;K$e$ Y$e OKaeOKN'ae$ OKeUe;yl:Nl'we$ lecOu:seYLea lYe u!qXdlyfe$ leNylFu:Ne:u$ eUPe!LqXCFObAVnuauae.OFlweUeYYye lse!Oy$ O:eKae.Oy'XWY'!qXXUmtped3tUmHXnl!Y:qXUPu:XDYYa$lye$Oe;l$$lXDOe leOKy'weY:'e;yO$lyse:l'qXXnGjU3oHXoOe$ GLeaflY$ HXD Y$eu:lale$yYLeK:fle Y..e$ lyeUecO!YLOaePOye!O:lXKae.lNaeOPe!Le'O!lel:Fl:e:l;Le-u!asXbllFKale-la$e:u$-lyeLfKyO:ye$ uaejOK'e:YFuye-u$cOK:'ea lXcla$.YTleYye$Oe:O$eNO$aePOy!eau$weUecY!l:aqXZOK!eOy:h'eu:ae'Y.u:NePyO!lqXXBU$eC3tjUHXnyY$ leO:lqXXAGjUmeD3oADimHXtO$!lyaeTOeKae-l..e-ul:haeY'u!eY:'KYy$e$Oe!YK'h'e$ leNY$ lye.O$.OKN qXXn3WUtU3HXXnlyale!lyl'efl$e-u$cNKse l'.leYyTlHe-l..l'e'yl:'aly'e.Yy$weLOKeaK$e!YLeLOKye lYPe!LeLOKe OK'e uae'O$ Le'OO'e$ uae!lyuN:fO$eU:$eu e!u:Nhau:NweY:$lLwXALe-O..e;le$ lqXXojUDHXD OheEOK:'eYyuacu:$seOPe-OaNeOKye;la$u.Ye lsedlY..u:Nea-uaa-..e-Oe lYFlePY$ OKyfKalyO!'KaeOyle;OO'e:O&e Y..e;le$OK:e;L$KykTIl:'HXRLe$ OK$l$K:$$lye uae'O-..O!lPeLOKePyO!e'lweLOK.'se le!Le.Y:'l.e-u..eLOKye lyle OZea$Le-O!e$ lefYa$lyle.O$ Zu:NKl'eu:eu:eYLHeaOLwelcFlyOK:$uu$eLKyaXKPefyOf'Ll'efYLYu'e lylPOyXUe'O-YyK:e lYyu:l'HXD l$eY:$aeOPe$ lea.uN $wea$uu.haeLOKya$eY.le$ lle$ylY:le ualXtO$qXXmWUbtj3tDUmHXoOTle lecYLu:eOKye'uawXR Y$ e'OO:wePOyebK:acl:Tle-u$ euKyePyu!u:awXXD lLeYyle$ lKc eOPe$yYKN eOPleYyuO:cle YLe O!eLOKX$ leNLOKereoKc l.aeY.'eau..eO$e:Oe$ le!leFuy$LqeY.YFuae:O;.uT:e.uTlebOFLHXRlyleYecOKffylaaqXXtADG?UmHXUeme3tWZAmeUDdAWCeUju..eOPe$ le:llyle!Y$al:XQOyeO:ae!uawXROyae$ lecO!f'lleuae YFleKKye OFl$weUe-O!Fu:N.l:He!LeUPleO$ lyeT:OK'rXXWbGB3mHeUeLOKe!uTle;K$ec Y.$eaO-lye'l!le.YFlweuPeOPeOPe Y:c-Yy!PlrXXd3tdAtBHXQYylh'e-ul:cleOPe!LeUeY!euaqXR Yyle$ leK:cO-:sXR Y$eKc Y:'e u!eOPe Y$ e$Oeh$uOPk lyeYOae-uY'eOPeOPePyLe$ OKe:Ok'e Y!we$ Y$efYuT:K;POye$ Y!eNOO'e$OeY:'ec;YfcOe.YFleZOKyeNLYywe:ON.'u:Ne$ lle!KylYy$ly$K;LwX;OO$ lye uae lalyle uae:YFlEKylqXXCocY;Lwe$ lellfu:Ne$ leaOK:eLOKeNyYu:NePOyfLecOe lOy.weLlKf$lyyaLe.u$ ea u:eYaevK$weY:'eNyOe:O$e$Oe;l!Lea$llaqXXBUtAenm?AvUAWHXDOe!O:l-lyle;KsXD Y$e;l$Leau:lXDOe'O!u:eLOKye.lY'Y'&e:OXe lfYNu$ qXXvWAWCmHX?YLqej Y$eafllawe- lecY!clweOPe Y:e YFlX$ Y$eUXA:'e;Y'wXA:'eoaha$eiOPeyu!ly$ylaqXXD u:ePuuaeOaLrXUeY:'ebKyleY..e$Y leuOe u!XRu..eLOKe:O$eaKyeu:OKye-Yea Y$ e O!lePyllyu:aLel:aeuyle;leu$e$ uaeNyO$lyqXXtG3te3WHXvK$Fl:wXD fO:Nel:'Oly-Y..eLlKye.u&e$Kae!Oylwe!K!lt.OFleaOe: Oe;l.Lwe-K$cOqXXCGUDUmHXR O-we$ le'Ya$lya.LweY:'Oyh'eOPLea Y$e$ lelyuNK$euN.u:NXmPe$ lenylYTlwXDOe;YNuNKawe$ u:e-k$ Y$lse$ lle-O: laecO!le O!ealYye$O.Y'weT:O-weaK-:e$ le!O:lyh.HXUe$Oy'e!Y.Y$clweaOe!Y:Fl:$l.eRla$e$Oe$ OKe!lVale;K$efylcOK wXPOyeK:$u:Ne$Y:laeu:e OK!eYfylaNe$YKe$OeY:$u..'LeNYFul.e YFlXD lY'eaOeflfKy'l$XQu.u$ u:lywXR -lcfeuae:O$e Yc"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tgen=ghostWriter(10000)\n",
    "detectPlagiarism(tgen, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
