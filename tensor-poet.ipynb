{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TextLibrary class: text library for training, encoding, batch generation,\n",
    "# and formatted source display\n",
    "class TextLibrary:\n",
    "    def __init__(self, filenames, max=100000000):\n",
    "        self.filenames = filenames\n",
    "        self.data=''\n",
    "        self.files=[]\n",
    "        index = 1\n",
    "        for filename in filenames:\n",
    "            fd={}\n",
    "            fd[\"name\"] = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.c2i = {}\n",
    "            self.i2c = {}\n",
    "            try:\n",
    "                f = open(filename)\n",
    "                dat = f.read(max)\n",
    "                self.data += dat\n",
    "                fd[\"data\"] = dat\n",
    "                fd[\"index\"] = index\n",
    "                index += 1\n",
    "                self.files.append(fd)\n",
    "                f.close()\n",
    "            except OSError:\n",
    "                print(\"  ERROR: Cannot read: \", filename)\n",
    "        ind = 0\n",
    "        for c in self.data: # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "            \n",
    "    def printColoredIPython(self, textlist, pre='', post=''):\n",
    "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n','<br>')\n",
    "            if ind==0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind%16]+\";\\\">\" + txt +\\\n",
    "                       \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "        \n",
    "    def sourceHighlight(self, txt, minQuoteSize=10):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc=[(\"Sources: \", 0)]\n",
    "        sc=False\n",
    "        noquote = ''\n",
    "        while len(tx)>0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p<=len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p<=len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1>mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f[\"name\"]\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote)>0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ],mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN,mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote)>0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.printColoredIPython(out)\n",
    "        if len(qts)>0:  # print references, if there is at least one source\n",
    "            self.printColoredIPython(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "    \n",
    "    def getSlice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "    \n",
    "    def decode(self, ar):\n",
    "         return ''.join([self.i2c[ic] for ic in ar])\n",
    "            \n",
    "    def getRandomSlice(self, length):\n",
    "        p = random.randrange(0,len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "    \n",
    "    def getSliceArray(self, length):\n",
    "        ar = np.array([c for c in self.getSlice(length)[0]])\n",
    "        return ar\n",
    "        \n",
    "    def getSample(self, length):\n",
    "        s, rst = self.getSlice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "    \n",
    "    def getRandomSample(self, length):\n",
    "        s = self.getRandomSlice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "    \n",
    "    def getSampleBatch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.getSample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "        \n",
    "    def getRandomSampleBatch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.getRandomSample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The tensorflow model for text generation\n",
    "class TensorPoetModel:\n",
    "    def __init__(self, params):\n",
    "        self.vocab_size = params[\"vocab_size\"]\n",
    "        self.neurons = params[\"neurons\"]\n",
    "        self.layers = params[\"layers\"]\n",
    "        self.learning_rate = params[\"learning_rate\"]\n",
    "        self.steps = params[\"steps\"]\n",
    "        # self.clip = -1.0 * params[\"clip\"]\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Training & Generating:\n",
    "        self.X = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
    "        self.y = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
    "\n",
    "        onehot_X = tf.one_hot(self.X, self.vocab_size)\n",
    "        onehot_y = tf.one_hot(self.y, self.vocab_size)\n",
    "\n",
    "        basic_cell = tf.contrib.rnn.BasicLSTMCell(self.neurons)\n",
    "        stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * self.layers)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32)\n",
    "        self.init_state_0 = stacked_cell.zero_state(self.batch_size, tf.float32)\n",
    "\n",
    "        self.init_state = self.init_state_0\n",
    "\n",
    "        with tf.variable_scope('rnn') as scope:\n",
    "            rnn_outputs, states = tf.nn.dynamic_rnn(stacked_cell, onehot_X, \n",
    "                                                    initial_state=self.init_state, \n",
    "                                                    dtype=tf.float32)\n",
    "            self.init_state = states\n",
    "\n",
    "        self.final_state = self.init_state\n",
    "        stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, self.neurons])\n",
    "\n",
    "        softmax_w = tf.Variable(tf.random_normal([self.neurons, self.vocab_size]), dtype=tf.float32)\n",
    "        softmax_b = tf.Variable([self.vocab_size], dtype=tf.float32)\n",
    "            \n",
    "        logits_raw = tf.matmul(stacked_rnn_outputs, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits_raw, [-1, self.steps, self.vocab_size])\n",
    "\n",
    "        output_softmax = tf.nn.softmax(logits)\n",
    "\n",
    "        self.temperature = tf.placeholder(tf.float32)\n",
    "        self.output_softmax_temp = tf.nn.softmax(tf.div(logits, self.temperature))\n",
    "\n",
    "        softmax_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=onehot_y, logits=logits)\n",
    "\n",
    "        self.cross_entropy = tf.reduce_mean(softmax_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "        self.training_op = optimizer.minimize(self.cross_entropy)\n",
    "        \n",
    "        # Clipping isn't necessary, even for really deep networks:\n",
    "        # grads = optimizer.compute_gradients(self.cross_entropy)\n",
    "        # minclip = -1.0 * self.clip\n",
    "        # capped_grads = [(tf.clip_by_value(grad, minclip, self.clip), var) for grad, var in grads]\n",
    "        # self.training_op = optimizer.apply_gradients(capped_grads)\n",
    "\n",
    "        self.prediction = tf.cast(tf.argmax(output_softmax, -1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.y, self.prediction)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        error = 1.0 - self.accuracy\n",
    "\n",
    "        \n",
    "        # Tensorboard\n",
    "        tf.summary.scalar(\"cross-entropy\", self.cross_entropy)\n",
    "        tf.summary.scalar(\"error\", error)\n",
    "        self.summary_merged = tf.summary.merge_all()\n",
    "\n",
    "        # Init\n",
    "        self.init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "textlib = TextLibrary([  # add additional texts, to train concurrently on multiple txts:\n",
    "                        # 'data/tiny-shakespeare.txt',\n",
    "                        # 'bk/emma-jane-austen.txt',\n",
    "                        # 'bk/voyage-out-virginia-woolf.txt',\n",
    "                        # 'bk/pride-prejudice-jane-austen.txt',\n",
    "                        # 'bk/wuthering-heights-emily-bronte.txt',            \n",
    "                        # 'bk/musil.txt',\n",
    "                        'bk/HauptwerkePhilosophieKonig.txt',\n",
    "                        'bk/HeideggerGeier.txt',\n",
    "                        'bk/HeideggerSafranski.txt',\n",
    "                        'bk/HeideggerTrawny.txt',\n",
    "                        'bk/MythenKonig.txt',\n",
    "                        'bk/phaenomenologie.txt',\n",
    "                        'bk/MiphamAdornment.txt',\n",
    "                        'bk/MiphamSelflessness.txt',\n",
    "                        'bk/MiphamBeacon.txt',\n",
    "                        'bk/MiphamSpeechDelight.txt',\n",
    "                        'bk/MiphamVividAwareness.txt',\n",
    "                        'bk/MiphamWhiteLotus.txt',\n",
    "                        'bk/HumeEnquiry.txt',\n",
    "                        'bk/KantKritik.txt',\n",
    "                        'bk/KantProlegomena.txt',\n",
    "                        'bk/McEvilleyShapeAncientThought.txt',\n",
    "                        'bk/PenroseEmperorNewMind.txt',\n",
    "                        'bk/PenroseLargeSmall.txt',\n",
    "                        'bk/AristotelesPhysik.txt',\n",
    "                        'bk/PlatonWerke.txt',\n",
    "                        'bk/schopenhauer.txt',\n",
    "                        'bk/SearleMindIntroduction.txt',\n",
    "                        'bk/SearleMindLanguageSociety.txt',\n",
    "                        'bk/SearleNeurosciencePhilosophy.txt',\n",
    "                        'bk/SpinozaEthik.txt',\n",
    "                      ])\n",
    "\n",
    "params = {\n",
    "    \"vocab_size\": len(textlib.i2c),\n",
    "    \"neurons\": 256,\n",
    "    \"layers\": 8,\n",
    "    \"learning_rate\": 1.e-3,\n",
    "    \"steps\": 128,\n",
    "}\n",
    "\n",
    "model = TensorPoetModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorboard: 'tensorboard --logdir /home/dsc/git/AI/tensor-poet/tensorlog'\n",
      "Restoring checkpoint at 51000: /home/dsc/git/AI/tensor-poet/tensorlog/model.ckpt-51000\n",
      "Epoch: 32.72, iter: 51000, cross-entropy: 0.958, accuracy: 0.70660\n",
      "Epoch: 32.85, iter: 51200, cross-entropy: 0.975, accuracy: 0.69525\n",
      "Epoch: 32.98, iter: 51400, cross-entropy: 1.000, accuracy: 0.69037\n",
      "Epoch: 33.11, iter: 51600, cross-entropy: 1.000, accuracy: 0.68475\n",
      "Epoch: 33.24, iter: 51800, cross-entropy: 0.951, accuracy: 0.70367\n",
      "Epoch: 33.36, iter: 52000, cross-entropy: 0.984, accuracy: 0.69806\n",
      "Epoch: 33.49, iter: 52200, cross-entropy: 0.976, accuracy: 0.69476\n",
      "Epoch: 33.62, iter: 52400, cross-entropy: 0.998, accuracy: 0.68982\n",
      "Epoch: 33.75, iter: 52600, cross-entropy: 0.963, accuracy: 0.70068\n",
      "Epoch: 33.88, iter: 52800, cross-entropy: 0.958, accuracy: 0.70160\n",
      "Epoch: 34.01, iter: 53000, cross-entropy: 0.973, accuracy: 0.70111\n",
      "Epoch: 34.13, iter: 53200, cross-entropy: 0.965, accuracy: 0.70319\n",
      "Epoch: 34.26, iter: 53400, cross-entropy: 0.974, accuracy: 0.69861\n",
      "Epoch: 34.39, iter: 53600, cross-entropy: 0.972, accuracy: 0.69965\n",
      "Epoch: 34.52, iter: 53800, cross-entropy: 0.999, accuracy: 0.69006\n",
      "Epoch: 34.65, iter: 54000, cross-entropy: 0.948, accuracy: 0.70428\n",
      "Epoch: 34.78, iter: 54200, cross-entropy: 0.968, accuracy: 0.70099\n",
      "Epoch: 34.90, iter: 54400, cross-entropy: 0.959, accuracy: 0.70410\n",
      "Epoch: 35.03, iter: 54600, cross-entropy: 0.967, accuracy: 0.70087\n",
      "Epoch: 35.16, iter: 54800, cross-entropy: 0.953, accuracy: 0.70831\n",
      "Epoch: 35.29, iter: 55000, cross-entropy: 0.969, accuracy: 0.70087\n",
      "Epoch: 35.42, iter: 55200, cross-entropy: 0.991, accuracy: 0.69885\n",
      "Epoch: 35.55, iter: 55400, cross-entropy: 0.964, accuracy: 0.70190\n",
      "Epoch: 35.67, iter: 55600, cross-entropy: 0.963, accuracy: 0.69666\n",
      "Epoch: 35.80, iter: 55800, cross-entropy: 0.961, accuracy: 0.70422\n",
      "Epoch: 35.93, iter: 56000, cross-entropy: 0.953, accuracy: 0.70447\n",
      "Epoch: 36.06, iter: 56200, cross-entropy: 0.970, accuracy: 0.69922\n",
      "Epoch: 36.19, iter: 56400, cross-entropy: 0.939, accuracy: 0.70886\n",
      "Epoch: 36.32, iter: 56600, cross-entropy: 0.986, accuracy: 0.69647\n",
      "Epoch: 36.44, iter: 56800, cross-entropy: 0.976, accuracy: 0.69989\n",
      "Epoch: 36.57, iter: 57000, cross-entropy: 0.960, accuracy: 0.70190\n",
      "Epoch: 36.70, iter: 57200, cross-entropy: 0.960, accuracy: 0.70142\n",
      "Epoch: 36.83, iter: 57400, cross-entropy: 0.948, accuracy: 0.70679\n",
      "Epoch: 36.96, iter: 57600, cross-entropy: 0.974, accuracy: 0.70312\n",
      "Epoch: 37.09, iter: 57800, cross-entropy: 0.954, accuracy: 0.70697\n",
      "Epoch: 37.21, iter: 58000, cross-entropy: 0.973, accuracy: 0.70166\n",
      "Epoch: 37.34, iter: 58200, cross-entropy: 0.961, accuracy: 0.70135\n",
      "Epoch: 37.47, iter: 58400, cross-entropy: 0.971, accuracy: 0.69922\n",
      "Epoch: 37.60, iter: 58600, cross-entropy: 0.963, accuracy: 0.70288\n",
      "Epoch: 37.73, iter: 58800, cross-entropy: 0.943, accuracy: 0.70905\n",
      "Epoch: 37.86, iter: 59000, cross-entropy: 0.985, accuracy: 0.69177\n",
      "Epoch: 37.98, iter: 59200, cross-entropy: 0.956, accuracy: 0.70392\n",
      "Epoch: 38.11, iter: 59400, cross-entropy: 0.946, accuracy: 0.70770\n",
      "Epoch: 38.24, iter: 59600, cross-entropy: 0.959, accuracy: 0.70416\n",
      "Epoch: 38.37, iter: 59800, cross-entropy: 0.951, accuracy: 0.70331\n",
      "Epoch: 38.50, iter: 60000, cross-entropy: 0.978, accuracy: 0.69806\n",
      "Epoch: 38.63, iter: 60200, cross-entropy: 0.963, accuracy: 0.70209\n",
      "Epoch: 38.75, iter: 60400, cross-entropy: 0.989, accuracy: 0.69745\n",
      "Epoch: 38.88, iter: 60600, cross-entropy: 0.935, accuracy: 0.70770\n",
      "Epoch: 39.01, iter: 60800, cross-entropy: 0.978, accuracy: 0.69568\n",
      "Epoch: 39.14, iter: 61000, cross-entropy: 0.944, accuracy: 0.70618\n",
      "Epoch: 39.27, iter: 61200, cross-entropy: 0.943, accuracy: 0.70697\n",
      "Epoch: 39.40, iter: 61400, cross-entropy: 0.971, accuracy: 0.70074\n",
      "Epoch: 39.52, iter: 61600, cross-entropy: 0.965, accuracy: 0.70166\n",
      "Epoch: 39.65, iter: 61800, cross-entropy: 0.962, accuracy: 0.70099\n",
      "Epoch: 39.78, iter: 62000, cross-entropy: 0.957, accuracy: 0.70239\n",
      "Epoch: 39.91, iter: 62200, cross-entropy: 0.987, accuracy: 0.69257\n",
      "Epoch: 40.04, iter: 62400, cross-entropy: 0.934, accuracy: 0.71069\n",
      "Epoch: 40.17, iter: 62600, cross-entropy: 0.952, accuracy: 0.70233\n",
      "Epoch: 40.29, iter: 62800, cross-entropy: 0.977, accuracy: 0.69379\n",
      "Epoch: 40.42, iter: 63000, cross-entropy: 0.955, accuracy: 0.70691\n",
      "Epoch: 40.55, iter: 63200, cross-entropy: 0.981, accuracy: 0.69781\n",
      "Epoch: 40.68, iter: 63400, cross-entropy: 0.934, accuracy: 0.71051\n",
      "Epoch: 40.81, iter: 63600, cross-entropy: 0.967, accuracy: 0.70258\n",
      "Epoch: 40.94, iter: 63800, cross-entropy: 0.970, accuracy: 0.69885\n",
      "Epoch: 41.06, iter: 64000, cross-entropy: 0.939, accuracy: 0.71021\n",
      "Epoch: 41.19, iter: 64200, cross-entropy: 0.970, accuracy: 0.70447\n",
      "Epoch: 41.32, iter: 64400, cross-entropy: 0.957, accuracy: 0.70129\n",
      "Epoch: 41.45, iter: 64600, cross-entropy: 0.959, accuracy: 0.70428\n",
      "Epoch: 41.58, iter: 64800, cross-entropy: 0.952, accuracy: 0.70496\n",
      "Epoch: 41.71, iter: 65000, cross-entropy: 0.941, accuracy: 0.71033\n",
      "Epoch: 41.83, iter: 65200, cross-entropy: 0.951, accuracy: 0.70459\n",
      "Epoch: 41.96, iter: 65400, cross-entropy: 0.958, accuracy: 0.70300\n",
      "Epoch: 42.09, iter: 65600, cross-entropy: 0.937, accuracy: 0.70569\n",
      "Epoch: 42.22, iter: 65800, cross-entropy: 0.954, accuracy: 0.70227\n",
      "Epoch: 42.35, iter: 66000, cross-entropy: 0.952, accuracy: 0.70538\n",
      "Epoch: 42.48, iter: 66200, cross-entropy: 0.963, accuracy: 0.70483\n",
      "Epoch: 42.60, iter: 66400, cross-entropy: 0.963, accuracy: 0.70404\n",
      "Epoch: 42.73, iter: 66600, cross-entropy: 0.977, accuracy: 0.69629\n",
      "Epoch: 42.86, iter: 66800, cross-entropy: 0.961, accuracy: 0.70319\n",
      "Epoch: 42.99, iter: 67000, cross-entropy: 0.985, accuracy: 0.69672\n",
      "Epoch: 43.12, iter: 67200, cross-entropy: 0.933, accuracy: 0.71124\n",
      "Epoch: 43.25, iter: 67400, cross-entropy: 0.923, accuracy: 0.71198\n",
      "Epoch: 43.37, iter: 67600, cross-entropy: 0.939, accuracy: 0.70776\n",
      "Epoch: 43.50, iter: 67800, cross-entropy: 0.972, accuracy: 0.69952\n",
      "Epoch: 43.63, iter: 68000, cross-entropy: 0.956, accuracy: 0.70300\n",
      "Epoch: 43.76, iter: 68200, cross-entropy: 0.946, accuracy: 0.70593\n",
      "Epoch: 43.89, iter: 68400, cross-entropy: 0.945, accuracy: 0.70630\n",
      "Epoch: 44.02, iter: 68600, cross-entropy: 0.972, accuracy: 0.69641\n",
      "Epoch: 44.14, iter: 68800, cross-entropy: 0.957, accuracy: 0.70447\n",
      "Epoch: 44.27, iter: 69000, cross-entropy: 0.958, accuracy: 0.70483\n",
      "Epoch: 44.40, iter: 69200, cross-entropy: 0.939, accuracy: 0.70807\n",
      "Epoch: 44.53, iter: 69400, cross-entropy: 0.933, accuracy: 0.70868\n",
      "Epoch: 44.66, iter: 69600, cross-entropy: 0.969, accuracy: 0.70044\n",
      "Epoch: 44.79, iter: 69800, cross-entropy: 0.940, accuracy: 0.70996\n",
      "Epoch: 44.91, iter: 70000, cross-entropy: 0.973, accuracy: 0.70264\n",
      "Epoch: 45.04, iter: 70200, cross-entropy: 0.935, accuracy: 0.70898\n",
      "Epoch: 45.17, iter: 70400, cross-entropy: 0.945, accuracy: 0.70563\n",
      "Epoch: 45.30, iter: 70600, cross-entropy: 0.926, accuracy: 0.71185\n",
      "Epoch: 45.43, iter: 70800, cross-entropy: 0.938, accuracy: 0.71033\n",
      "Epoch: 45.56, iter: 71000, cross-entropy: 0.961, accuracy: 0.70496\n",
      "Epoch: 45.68, iter: 71200, cross-entropy: 0.963, accuracy: 0.70068\n",
      "Epoch: 45.81, iter: 71400, cross-entropy: 0.981, accuracy: 0.69910\n",
      "Epoch: 45.94, iter: 71600, cross-entropy: 0.943, accuracy: 0.71234\n",
      "Epoch: 46.07, iter: 71800, cross-entropy: 0.974, accuracy: 0.70343\n",
      "Epoch: 46.20, iter: 72000, cross-entropy: 0.936, accuracy: 0.70520\n",
      "Epoch: 46.33, iter: 72200, cross-entropy: 0.946, accuracy: 0.70844\n",
      "Epoch: 46.45, iter: 72400, cross-entropy: 0.959, accuracy: 0.70282\n",
      "Epoch: 46.58, iter: 72600, cross-entropy: 0.938, accuracy: 0.70959\n",
      "Epoch: 46.71, iter: 72800, cross-entropy: 0.959, accuracy: 0.70435\n",
      "Epoch: 46.84, iter: 73000, cross-entropy: 0.955, accuracy: 0.70477\n",
      "Epoch: 46.97, iter: 73200, cross-entropy: 0.950, accuracy: 0.70563\n",
      "Epoch: 47.10, iter: 73400, cross-entropy: 0.965, accuracy: 0.70044\n",
      "Epoch: 47.22, iter: 73600, cross-entropy: 0.945, accuracy: 0.70386\n",
      "Epoch: 47.35, iter: 73800, cross-entropy: 0.943, accuracy: 0.70862\n",
      "Epoch: 47.48, iter: 74000, cross-entropy: 0.963, accuracy: 0.69904\n",
      "Epoch: 47.61, iter: 74200, cross-entropy: 0.966, accuracy: 0.70386\n",
      "Epoch: 47.74, iter: 74400, cross-entropy: 0.961, accuracy: 0.70325\n",
      "Epoch: 47.87, iter: 74600, cross-entropy: 0.962, accuracy: 0.70190\n",
      "Epoch: 47.99, iter: 74800, cross-entropy: 0.967, accuracy: 0.70032\n",
      "Epoch: 48.12, iter: 75000, cross-entropy: 0.927, accuracy: 0.70685\n",
      "Epoch: 48.25, iter: 75200, cross-entropy: 0.948, accuracy: 0.70551\n",
      "Epoch: 48.38, iter: 75400, cross-entropy: 0.963, accuracy: 0.70288\n",
      "Epoch: 48.51, iter: 75600, cross-entropy: 0.939, accuracy: 0.71002\n",
      "Epoch: 48.63, iter: 75800, cross-entropy: 0.951, accuracy: 0.70764\n",
      "Epoch: 48.76, iter: 76000, cross-entropy: 0.960, accuracy: 0.70538\n",
      "Epoch: 48.89, iter: 76200, cross-entropy: 0.956, accuracy: 0.70386\n",
      "Epoch: 49.02, iter: 76400, cross-entropy: 0.961, accuracy: 0.70020\n",
      "Epoch: 49.15, iter: 76600, cross-entropy: 0.960, accuracy: 0.70422\n",
      "Epoch: 49.28, iter: 76800, cross-entropy: 0.951, accuracy: 0.70465\n",
      "Epoch: 49.40, iter: 77000, cross-entropy: 0.912, accuracy: 0.71600\n",
      "Epoch: 49.53, iter: 77200, cross-entropy: 0.922, accuracy: 0.71344\n",
      "Epoch: 49.66, iter: 77400, cross-entropy: 0.954, accuracy: 0.70300\n",
      "Epoch: 49.79, iter: 77600, cross-entropy: 0.932, accuracy: 0.70953\n",
      "Epoch: 49.92, iter: 77800, cross-entropy: 0.950, accuracy: 0.70789\n",
      "Epoch: 50.05, iter: 78000, cross-entropy: 0.939, accuracy: 0.70728\n",
      "Epoch: 50.17, iter: 78200, cross-entropy: 0.943, accuracy: 0.70886\n",
      "Epoch: 50.30, iter: 78400, cross-entropy: 0.933, accuracy: 0.71167\n",
      "Epoch: 50.43, iter: 78600, cross-entropy: 0.941, accuracy: 0.70648\n",
      "Epoch: 50.56, iter: 78800, cross-entropy: 0.962, accuracy: 0.70569\n",
      "Epoch: 50.69, iter: 79000, cross-entropy: 0.949, accuracy: 0.70770\n",
      "Epoch: 50.82, iter: 79200, cross-entropy: 0.935, accuracy: 0.71075\n",
      "Epoch: 50.94, iter: 79400, cross-entropy: 0.941, accuracy: 0.70581\n",
      "Epoch: 51.07, iter: 79600, cross-entropy: 0.955, accuracy: 0.70441\n",
      "Epoch: 51.20, iter: 79800, cross-entropy: 0.921, accuracy: 0.71289\n",
      "Epoch: 51.33, iter: 80000, cross-entropy: 0.933, accuracy: 0.71008\n",
      "Epoch: 51.46, iter: 80200, cross-entropy: 0.928, accuracy: 0.70404\n",
      "Epoch: 51.59, iter: 80400, cross-entropy: 0.945, accuracy: 0.70422\n",
      "Epoch: 51.71, iter: 80600, cross-entropy: 0.926, accuracy: 0.71118\n",
      "Epoch: 51.84, iter: 80800, cross-entropy: 0.952, accuracy: 0.70599\n",
      "Epoch: 51.97, iter: 81000, cross-entropy: 0.957, accuracy: 0.70312\n",
      "Epoch: 52.10, iter: 81200, cross-entropy: 0.972, accuracy: 0.69733\n",
      "Epoch: 52.23, iter: 81400, cross-entropy: 0.929, accuracy: 0.70862\n",
      "Epoch: 52.36, iter: 81600, cross-entropy: 0.944, accuracy: 0.70801\n",
      "Epoch: 52.48, iter: 81800, cross-entropy: 0.932, accuracy: 0.70972\n",
      "Epoch: 52.61, iter: 82000, cross-entropy: 0.923, accuracy: 0.71393\n",
      "Epoch: 52.74, iter: 82200, cross-entropy: 0.916, accuracy: 0.71460\n",
      "Epoch: 52.87, iter: 82400, cross-entropy: 0.929, accuracy: 0.70990\n",
      "Epoch: 53.00, iter: 82600, cross-entropy: 0.926, accuracy: 0.71277\n",
      "Epoch: 53.13, iter: 82800, cross-entropy: 0.944, accuracy: 0.70569\n",
      "Epoch: 53.25, iter: 83000, cross-entropy: 0.933, accuracy: 0.71002\n",
      "Epoch: 53.38, iter: 83200, cross-entropy: 0.935, accuracy: 0.71344\n",
      "Epoch: 53.51, iter: 83400, cross-entropy: 0.975, accuracy: 0.69592\n",
      "Epoch: 53.64, iter: 83600, cross-entropy: 0.948, accuracy: 0.70599\n",
      "Epoch: 53.77, iter: 83800, cross-entropy: 0.951, accuracy: 0.70038\n",
      "Epoch: 53.90, iter: 84000, cross-entropy: 0.951, accuracy: 0.71051\n",
      "Epoch: 54.02, iter: 84200, cross-entropy: 0.944, accuracy: 0.70697\n",
      "Epoch: 54.15, iter: 84400, cross-entropy: 0.920, accuracy: 0.71484\n",
      "Epoch: 54.28, iter: 84600, cross-entropy: 0.936, accuracy: 0.71173\n",
      "Epoch: 54.41, iter: 84800, cross-entropy: 0.932, accuracy: 0.70892\n",
      "Epoch: 54.54, iter: 85000, cross-entropy: 0.949, accuracy: 0.70905\n",
      "Epoch: 54.67, iter: 85200, cross-entropy: 0.951, accuracy: 0.70239\n",
      "Epoch: 54.79, iter: 85400, cross-entropy: 0.949, accuracy: 0.70874\n",
      "Epoch: 54.92, iter: 85600, cross-entropy: 0.922, accuracy: 0.71851\n",
      "Epoch: 55.05, iter: 85800, cross-entropy: 0.922, accuracy: 0.71252\n",
      "Epoch: 55.18, iter: 86000, cross-entropy: 0.961, accuracy: 0.70398\n",
      "Epoch: 55.31, iter: 86200, cross-entropy: 0.937, accuracy: 0.70850\n",
      "Epoch: 55.44, iter: 86400, cross-entropy: 0.945, accuracy: 0.70532\n",
      "Epoch: 55.56, iter: 86600, cross-entropy: 0.947, accuracy: 0.70752\n",
      "Epoch: 55.69, iter: 86800, cross-entropy: 0.945, accuracy: 0.70642\n",
      "Epoch: 55.82, iter: 87000, cross-entropy: 0.952, accuracy: 0.70270\n",
      "Epoch: 55.95, iter: 87200, cross-entropy: 0.937, accuracy: 0.70929\n",
      "Epoch: 56.08, iter: 87400, cross-entropy: 0.949, accuracy: 0.71008\n",
      "Epoch: 56.21, iter: 87600, cross-entropy: 0.945, accuracy: 0.71228\n",
      "Epoch: 56.33, iter: 87800, cross-entropy: 0.925, accuracy: 0.71289\n",
      "Epoch: 56.46, iter: 88000, cross-entropy: 0.946, accuracy: 0.70801\n",
      "Epoch: 56.59, iter: 88200, cross-entropy: 0.929, accuracy: 0.71198\n",
      "Epoch: 56.72, iter: 88400, cross-entropy: 0.921, accuracy: 0.71503\n",
      "Epoch: 56.85, iter: 88600, cross-entropy: 0.915, accuracy: 0.71802\n",
      "Epoch: 56.98, iter: 88800, cross-entropy: 0.926, accuracy: 0.71118\n",
      "Epoch: 57.10, iter: 89000, cross-entropy: 0.910, accuracy: 0.71619\n",
      "Epoch: 57.23, iter: 89200, cross-entropy: 0.952, accuracy: 0.70361\n",
      "Epoch: 57.36, iter: 89400, cross-entropy: 0.956, accuracy: 0.70349\n",
      "Epoch: 57.49, iter: 89600, cross-entropy: 0.918, accuracy: 0.71344\n",
      "Epoch: 57.62, iter: 89800, cross-entropy: 0.925, accuracy: 0.71753\n",
      "Epoch: 57.75, iter: 90000, cross-entropy: 0.921, accuracy: 0.71741\n",
      "Epoch: 57.87, iter: 90200, cross-entropy: 0.958, accuracy: 0.70483\n",
      "Epoch: 58.00, iter: 90400, cross-entropy: 0.920, accuracy: 0.71643\n",
      "Epoch: 58.13, iter: 90600, cross-entropy: 0.926, accuracy: 0.71167\n",
      "Epoch: 58.26, iter: 90800, cross-entropy: 0.952, accuracy: 0.70331\n",
      "Epoch: 58.39, iter: 91000, cross-entropy: 0.914, accuracy: 0.72034\n",
      "Epoch: 58.52, iter: 91200, cross-entropy: 0.935, accuracy: 0.71155\n",
      "Epoch: 58.64, iter: 91400, cross-entropy: 0.929, accuracy: 0.71338\n",
      "Epoch: 58.77, iter: 91600, cross-entropy: 0.943, accuracy: 0.70508\n",
      "Epoch: 58.90, iter: 91800, cross-entropy: 0.929, accuracy: 0.71350\n",
      "Epoch: 59.03, iter: 92000, cross-entropy: 0.940, accuracy: 0.70624\n",
      "Epoch: 59.16, iter: 92200, cross-entropy: 0.933, accuracy: 0.70770\n",
      "Epoch: 59.29, iter: 92400, cross-entropy: 0.942, accuracy: 0.70636\n",
      "Epoch: 59.41, iter: 92600, cross-entropy: 0.954, accuracy: 0.70276\n",
      "Epoch: 59.54, iter: 92800, cross-entropy: 0.903, accuracy: 0.71698\n",
      "Epoch: 59.67, iter: 93000, cross-entropy: 0.924, accuracy: 0.71271\n",
      "Epoch: 59.80, iter: 93200, cross-entropy: 0.920, accuracy: 0.71539\n",
      "Epoch: 59.93, iter: 93400, cross-entropy: 0.939, accuracy: 0.70959\n",
      "Epoch: 60.06, iter: 93600, cross-entropy: 0.913, accuracy: 0.71722\n",
      "Epoch: 60.18, iter: 93800, cross-entropy: 0.939, accuracy: 0.70898\n",
      "Epoch: 60.31, iter: 94000, cross-entropy: 0.904, accuracy: 0.71899\n",
      "Epoch: 60.44, iter: 94200, cross-entropy: 0.950, accuracy: 0.70740\n",
      "Epoch: 60.57, iter: 94400, cross-entropy: 0.927, accuracy: 0.71344\n",
      "Epoch: 60.70, iter: 94600, cross-entropy: 0.975, accuracy: 0.69977\n",
      "Epoch: 60.83, iter: 94800, cross-entropy: 0.907, accuracy: 0.71747\n",
      "Epoch: 60.95, iter: 95000, cross-entropy: 0.914, accuracy: 0.71674\n",
      "Epoch: 61.08, iter: 95200, cross-entropy: 0.943, accuracy: 0.70819\n",
      "Epoch: 61.21, iter: 95400, cross-entropy: 0.930, accuracy: 0.70868\n",
      "Epoch: 61.34, iter: 95600, cross-entropy: 0.942, accuracy: 0.70844\n",
      "Epoch: 61.47, iter: 95800, cross-entropy: 0.930, accuracy: 0.71002\n",
      "Epoch: 61.60, iter: 96000, cross-entropy: 0.939, accuracy: 0.70593\n",
      "Epoch: 61.72, iter: 96200, cross-entropy: 0.930, accuracy: 0.71545\n",
      "Epoch: 61.85, iter: 96400, cross-entropy: 0.935, accuracy: 0.71259\n",
      "Epoch: 61.98, iter: 96600, cross-entropy: 0.902, accuracy: 0.72064\n",
      "Epoch: 62.11, iter: 96800, cross-entropy: 0.938, accuracy: 0.70892\n",
      "Epoch: 62.24, iter: 97000, cross-entropy: 0.944, accuracy: 0.70813\n",
      "Epoch: 62.37, iter: 97200, cross-entropy: 0.934, accuracy: 0.71014\n",
      "Epoch: 62.49, iter: 97400, cross-entropy: 0.923, accuracy: 0.71826\n",
      "Epoch: 62.62, iter: 97600, cross-entropy: 0.912, accuracy: 0.71655\n",
      "Epoch: 62.75, iter: 97800, cross-entropy: 0.936, accuracy: 0.71289\n",
      "Epoch: 62.88, iter: 98000, cross-entropy: 0.931, accuracy: 0.70862\n",
      "Epoch: 63.01, iter: 98200, cross-entropy: 0.915, accuracy: 0.71960\n",
      "Epoch: 63.14, iter: 98400, cross-entropy: 0.941, accuracy: 0.71173\n",
      "Epoch: 63.26, iter: 98600, cross-entropy: 0.939, accuracy: 0.70691\n"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "max_iter = 1000000\n",
    "batch_size = 128\n",
    "generated_text_size = 500\n",
    "restoreCheckpoints = True\n",
    "generateDuringTraining = False\n",
    "verbose = False\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    epl = len(textlib.data) / (batch_size * model.steps)\n",
    "    model.init.run()\n",
    "\n",
    "    tflogdir = os.path.realpath('tensorlog')\n",
    "    if not os.path.exists(tflogdir):\n",
    "        os.makedirs(tflogdir)\n",
    "    print(\"Tensorboard: 'tensorboard --logdir {}'\".format(tflogdir))\n",
    "\n",
    "    train_writer = tf.summary.FileWriter(tflogdir, sess.graph)\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    \n",
    "    # Used for saving the training parameters periodically\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint_file = os.path.join(tflogdir, 'model.ckpt')\n",
    "    # FFR: tf.train.export_meta_graph(filename=None, meta_info_def=None, graph_def=None, saver_def=None, collection_list=None, as_text=False, graph=None, export_scope=None, clear_devices=False, **kwargs)\n",
    "    start_iter = 0\n",
    "    if restoreCheckpoints:\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter=int(lastSave[pt:])\n",
    "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "    \n",
    "    for iteration in range(start_iter, max_iter):\n",
    "        # Train with batches from the text library:\n",
    "        X_batch, y_batch = textlib.getRandomSampleBatch(batch_size, model.steps)\n",
    "        i_state = sess.run([model.init_state_0], feed_dict={model.batch_size: batch_size})\n",
    "        i_state, _ = sess.run([model.final_state, model.training_op],\n",
    "                              feed_dict={model.X: X_batch, model.y: y_batch,\n",
    "                                         model.batch_size: batch_size, model.init_state: i_state})\n",
    "\n",
    "        # Output training statistics every 100 iterations:\n",
    "        if iteration % 200 == 0:\n",
    "            ce, accuracy, prediction, summary = sess.run([model.cross_entropy,\n",
    "                                                          model.accuracy, model.prediction,\n",
    "                                                          model.summary_merged],\n",
    "                                             feed_dict={model.X: X_batch, model.y: y_batch,\n",
    "                                                        model.batch_size: batch_size})\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            ep = iteration / epl\n",
    "            print(\"Epoch: {0:.2f}, iter: {1:d}, cross-entropy: {2:.3f}, accuracy: {3:.5f}\".format(ep, iteration, ce, accuracy))\n",
    "            if verbose:\n",
    "                for ind in range(1): # model.batch_size):\n",
    "                    ys = textlib.decode(y_batch[ind]).replace('\\n', ' | ')\n",
    "                    yps = textlib.decode(prediction[ind]).replace('\\n', ' | ')\n",
    "                    print(\"   y:\", ys)\n",
    "                    print(\"  yp:\", yps)\n",
    "\n",
    "        # Generate sample texts for different temperature every 500 iterations:\n",
    "        if (iteration+1) % 500 == 0:\n",
    "            \n",
    "            # Save training data\n",
    "            saver.save(sess, checkpoint_file, global_step=iteration+1)\n",
    "\n",
    "            if generateDuringTraining:\n",
    "                # Generate sample\n",
    "                for t in range(2, 11, 4):\n",
    "                    temp = float(t) / 10.0;\n",
    "                    g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "                    xs = ' ' * model.steps\n",
    "                    xso = ''\n",
    "                    for i in range(generated_text_size):\n",
    "                        X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                        g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp], \n",
    "                                                  feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                             model.batch_size: 1, model.temperature: temp})\n",
    "                        inds=list(range(model.vocab_size))\n",
    "                        ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                        nc = textlib.i2c[ind]\n",
    "                        xso += nc\n",
    "                        xs = xs[1:]+nc\n",
    "\n",
    "                    print(\"----------------- temperature =\", temp, \"----------------------\")\n",
    "                    # print(xso)\n",
    "                    textlib.sourceHighlight(xso, 20)   # 20: minimum quote size detected.\n",
    "                print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating text using the model data generated during training.\n",
    "def ghostWriter(textsize, temperature=1.0):\n",
    "    xso = None\n",
    "    with tf.Session() as sess:\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath('tensorlog')\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(tflogdir))\n",
    "            return None\n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, 'model.ckpt')\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter=int(lastSave[pt:])\n",
    "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
    "            return None\n",
    "\n",
    "        g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "        xs = ' ' * model.steps\n",
    "        xso = ''\n",
    "        for i in range(textsize):\n",
    "            X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "            g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp], \n",
    "                                      feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                 model.batch_size: 1, model.temperature: temperature})\n",
    "            inds=list(range(model.vocab_size))\n",
    "            ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "            nc = textlib.i2c[ind]\n",
    "            xso += nc\n",
    "            xs = xs[1:]+nc\n",
    "    return(xso)\n",
    "\n",
    "\n",
    "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.sourceHighlight(generatedtext, minQuoteLength)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tgen=ghostWriter(10000)\n",
    "detectPlagiarism(tgen, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2, maxAnswerSize=512, temperature=1.0):\n",
    "def doDialog():\n",
    "    temperature = 0.6  # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    maxEndPrompts = 4  # look for number of maxEndPrompts until answer is finished.\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Please enter some dialog.\")\n",
    "        print(\"The net will answer according to your input.\")\n",
    "        print(\"'bye' for end,\")\n",
    "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "        print(\"    to change character of the dialog.\")\n",
    "        print(\"    Current temperature={}.\".format(temperature))\n",
    "        print()\n",
    "        xso = None\n",
    "        bye = False\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath('tensorlog')\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(tflogdir))\n",
    "            return \n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, 'model.ckpt')\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter=int(lastSave[pt:])\n",
    "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
    "            return\n",
    "\n",
    "        g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "\n",
    "        bye = False\n",
    "        while not bye:\n",
    "            print(\"> \", end=\"\")\n",
    "            prompt = input()\n",
    "            if prompt == 'bye':\n",
    "                bye = True\n",
    "                print(\"Good bye!\")\n",
    "                continue\n",
    "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
    "                t = float(prompt[len(\"temperature=\"):])\n",
    "                if t>0.05 and t<1.4:\n",
    "                    temperature = t\n",
    "                    print(\"(generator temperature now {})\".format(t))\n",
    "                    print()\n",
    "                    continue\n",
    "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
    "                continue\n",
    "            xs = ' ' * model.steps\n",
    "            xso = ''\n",
    "            for rep in range(3):\n",
    "                for i in range(len(prompt)):\n",
    "                    xs = xs[1:]+prompt[i]\n",
    "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp], \n",
    "                                              feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                         model.batch_size: 1, model.temperature: temperature})\n",
    "            ans=0\n",
    "            numEndPrompts = 0\n",
    "            while (ans<maxAnswerSize and numEndPrompts < maxEndPrompts) or ans<minAnswerSize:\n",
    "\n",
    "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp], \n",
    "                                          feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                     model.batch_size: 1, model.temperature: temperature})\n",
    "                inds=list(range(model.vocab_size))\n",
    "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                nc = textlib.i2c[ind]\n",
    "                if nc == endPrompt:\n",
    "                    numEndPrompts += 1\n",
    "                xso += nc\n",
    "                xs = xs[1:]+nc\n",
    "                ans += 1\n",
    "            textlib.sourceHighlight(xso, 13)\n",
    "            # print(xso.replace(\"\\\\n\",\"\\n\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter some dialog.\n",
      "The net will answer according to your input.\n",
      "'bye' for end,\n",
      "'temperature=<float>' [0.1(frozen)-1.0(creative)]\n",
      "    to change character of the dialog.\n",
      "    Current temperature=0.6.\n",
      "> Jou hat mir gesagt, daß Materialismus keine haltbare philosophische Position ist. Stimmt das?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " D<span style=\"background-color:#eadbd8;\">ie Grenze der Philosophie </span><sup>[3]</sup><span style=\"background-color:#edebd0;\">erkennen in der </span><sup>[21]</sup>al<span style=\"background-color:#e5e8e8;\">ten Tafel der </span><sup>[15]</sup><span style=\"background-color:#d8daef;\">Erkenntnis, n</span><sup>[1]</sup><span style=\"background-color:#fdebd0;\">ach welchem das </span><sup>[14]</sup><span style=\"background-color:#eadbd8;\">Unerkennbare </span><sup>[19]</sup><span style=\"background-color:#ebdef0;\">die einzelne </span><sup>[2]</sup><span style=\"background-color:#ecf3cf;\">reine Einheit </span><sup>[6]</sup><span style=\"background-color:#edebd0;\">hat, oder der </span><sup>[21]</sup><span style=\"background-color:#ecf3cf;\">Bewegung des Bewußtseins und d</span><sup>[6]</sup><span style=\"background-color:#eadbd8;\">es Seins, das </span><sup>[3]</sup><span style=\"background-color:#ecf3cf;\">andremal ist </span><sup>[6]</sup><span style=\"background-color:#ecf3cf;\">die reale Welt</span><sup>[6]</sup><span style=\"background-color:#ecf3cf;\">. Diese Bestimmtheit </span><sup>[6]</sup><span style=\"background-color:#eadbd8;\">und die einzelne </span><sup>[3]</sup><span style=\"background-color:#ecf3cf;\">Substanz ist die </span><sup>[6]</sup><span style=\"background-color:#ecf3cf;\">Gewißheit seiner selbst, die </span><sup>[6]</sup><span style=\"background-color:#ecf3cf;\">durch das Selbst </span><sup>[6]</sup><span style=\"background-color:#ecf3cf;\">und die Notwendigkeit des </span><sup>[6]</sup><span style=\"background-color:#ecf3cf;\">Bewußtseins in sich selbst </span><sup>[6]</sup>(intellektu)<span style=\"background-color:#eadbd8;\">, der sich ent</span><sup>[3]</sup><span style=\"background-color:#edebd0;\">gegengesetzt ist, und </span><sup>[21]</sup><span style=\"background-color:#ecf3cf;\">das Wesen sich a</span><sup>[6]</sup><span style=\"background-color:#eadbd8;\">us dem Nichtseienden </span><sup>[19]</sup><span style=\"background-color:#e2d7d5;\">verschmelzen, </span><sup>[20]</sup><span style=\"background-color:#ecf3cf;\">und das gleich</span><sup>[6]</sup>e<span style=\"background-color:#ecf3cf;\"> Wesen in sich </span><sup>[6]</sup><span style=\"background-color:#ecf3cf;\">selbst, und dieser </span><sup>[6]</sup><span style=\"background-color:#d8daef;\">im Verstehen </span><sup>[1]</sup><span style=\"background-color:#d6eaf8;\">angenommen wird, so </span><sup>[25]</sup><span style=\"background-color:#e2d7d5;\">verschieden ist die</span><sup>[20]</sup><span style=\"background-color:#eadbd8;\"> Bewegung, der Z</span><sup>[19]</sup><span style=\"background-color:#eadbd8;\">ahl nach diese</span><sup>[19]</sup><span style=\"background-color:#e2d7d5;\">s Wesen als das </span><sup>[4]</sup>Entzwei<span style=\"background-color:#d6eaf8;\">te des Denkens und </span><sup>[25]</sup>jedes T<span style=\"background-color:#eadbd8;\">eils in sich selbst z</span><sup>[19]</sup><span style=\"background-color:#eadbd8;\">u sein, und das </span><sup>[3]</sup><span style=\"background-color:#ecf3cf;\">Ganze, welches </span><sup>[6]</sup><span style=\"background-color:#ecf3cf;\">in der Tat na</span><sup>[6]</sup><span style=\"background-color:#fdebd0;\">türlich ist, </span><sup>[14]</sup><span style=\"background-color:#ecf3cf;\">oder daß es ein</span><sup>[6]</sup><span style=\"background-color:#eadbd8;\">e Verschiedenheit der </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">Zeit bestimmte </span><sup>[19]</sup>ist,<span style=\"background-color:#e2d7d5;\"> nicht, wenn es d</span><sup>[20]</sup><span style=\"background-color:#ebdef0;\">er wesentliche Gegenstand </span><sup>[2]</sup><span style=\"background-color:#edebd0;\">des innern Wesens der Welt</span><sup>[21]</sup><span style=\"background-color:#ecf3cf;\"> ist, so ist der Geist</span><sup>[6]</sup><span style=\"background-color:#eadbd8;\"> des Wirklichen </span><sup>[3]</sup>(<span style=\"background-color:#edebd0;\">im Selbstbewußtseyn</span><sup>[21]</sup><span style=\"background-color:#edebd0;\">) und das Ver</span><sup>[21]</sup>mö<span style=\"background-color:#e2d7d5;\">gen des Scheins</span><sup>[20]</sup><span style=\"background-color:#edebd0;\">, in welchem sie i</span><sup>[21]</sup><span style=\"background-color:#fdebd0;\">m Raume anzutreffen s</span><sup>[14]</sup>ei<span style=\"background-color:#eadbd8;\">. Es ist aber die </span><sup>[19]</sup><span style=\"background-color:#edebd0;\">Todesfurcht de</span><sup>[21]</sup><span style=\"background-color:#edebd0;\">r Freiheit, we</span><sup>[21]</sup>lche<span style=\"background-color:#fdebd0;\"> das Glück des </span><sup>[14]</sup><span style=\"background-color:#ecf3cf;\">Wissens von d</span><sup>[6]</sup><span style=\"background-color:#eadbd8;\">iesen beiden ein</span><sup>[19]</sup><span style=\"background-color:#ecf3cf;\">zelnen Seiten </span><sup>[6]</sup><span style=\"background-color:#eadbd8;\">nur von der E</span><sup>[3]</sup><span style=\"background-color:#edebd0;\">rscheinung des Willens </span><sup>[21]</sup>habe<span style=\"background-color:#edebd0;\">, endlich und </span><sup>[21]</sup><span style=\"background-color:#e2d7d5;\">geschieht. Die</span><sup>[4]</sup>s<span style=\"background-color:#eadbd8;\">e haben sich </span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">in der Geschichtswissenschaft </span><sup>[2]</sup><span style=\"background-color:#edebd0;\">auf den Stand</span><sup>[21]</sup> zu bringen."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">HeideggerSafranski</span><sup>[3]</sup>, <span style=\"background-color:#edebd0;\">schopenhauer</span><sup>[21]</sup>, <span style=\"background-color:#e5e8e8;\">KantProlegomena</span><sup>[15]</sup>, <span style=\"background-color:#d8daef;\">HauptwerkePhilosophieKonig</span><sup>[1]</sup>, <span style=\"background-color:#fdebd0;\">KantKritik</span><sup>[14]</sup>, <span style=\"background-color:#eadbd8;\">AristotelesPhysik</span><sup>[19]</sup>, <span style=\"background-color:#ebdef0;\">HeideggerGeier</span><sup>[2]</sup>, <span style=\"background-color:#ecf3cf;\">phaenomenologie</span><sup>[6]</sup>, <span style=\"background-color:#e2d7d5;\">PlatonWerke</span><sup>[20]</sup>, <span style=\"background-color:#d6eaf8;\">SpinozaEthik</span><sup>[25]</sup>, <span style=\"background-color:#e2d7d5;\">HeideggerTrawny</span><sup>[4]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Jou told me that materialism is not a tenable philosphical position. Is that correct?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#d6eaf8;\"> While this is a </span><sup>[9]</sup><span style=\"background-color:#d6eaf8;\">single phenomenon </span><sup>[9]</sup><span style=\"background-color:#d8daef;\">that could be p</span><sup>[17]</sup><span style=\"background-color:#d4efdf;\">roduced on the </span><sup>[7]</sup>pa<span style=\"background-color:#d0ece7;\">th. There is nothing </span><sup>[24]</sup>i<span style=\"background-color:#d4efdf;\">n fact, they </span><sup>[7]</sup>consis<span style=\"background-color:#d8daef;\">t of our minds, </span><sup>[17]</sup>i<span style=\"background-color:#ecf3cf;\">t is lost in th</span><sup>[22]</sup>is<span style=\"background-color:#d6dbdf;\"> way. We do not </span><sup>[11]</sup><span style=\"background-color:#d8daef;\">know what the </span><sup>[17]</sup><span style=\"background-color:#ecf3cf;\">nonexistence of the mind</span><sup>[22]</sup><span style=\"background-color:#d4e6f1;\"> is universal </span><sup>[16]</sup><span style=\"background-color:#d6eaf8;\">and the other te</span><sup>[9]</sup>rms<span style=\"background-color:#ecf3cf;\">, we are describing </span><sup>[22]</sup><span style=\"background-color:#d4e6f1;\">the inference of a</span><sup>[16]</sup><span style=\"background-color:#d6eaf8;\">n interpretation of the </span><sup>[9]</sup>other and useful<span style=\"background-color:#d4e6f1;\"> complex process</span><sup>[16]</sup><span style=\"background-color:#d4e6f1;\">es in the sou</span><sup>[16]</sup>rces<span style=\"background-color:#d4e6f1;\"> of a concordant </span><sup>[10]</sup>and long a<span style=\"background-color:#d4efdf;\">s of the observe</span><sup>[7]</sup><span style=\"background-color:#d6dbdf;\">r dependent origination</span><sup>[11]</sup><span style=\"background-color:#d4efdf;\"> that is produced</span><sup>[7]</sup> by<span style=\"background-color:#d4efdf;\"> a truly existent </span><sup>[7]</sup><span style=\"background-color:#d4e6f1;\">nature is not p</span><sup>[10]</sup><span style=\"background-color:#d4e6f1;\">resent. The c</span><sup>[10]</sup><span style=\"background-color:#d4efdf;\">ause for the a</span><sup>[7]</sup><span style=\"background-color:#d6dbdf;\">nalytic meditation of the </span><sup>[11]</sup><span style=\"background-color:#d4e6f1;\">apprehension of the si</span><sup>[10]</sup>g<span style=\"background-color:#d4e6f1;\">n of the body </span><sup>[16]</sup><span style=\"background-color:#d6dbdf;\">of the extremes of </span><sup>[11]</sup><span style=\"background-color:#d4efdf;\">nonexistence and </span><sup>[7]</sup>a brai<span style=\"background-color:#d6eaf8;\">n constitute </span><sup>[9]</sup><span style=\"background-color:#d4e6f1;\">something separate from the </span><sup>[16]</sup><span style=\"background-color:#d4e6f1;\">experience of consciousness</span><sup>[16]</sup>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#d6eaf8;\">MiphamBeacon</span><sup>[9]</sup>, <span style=\"background-color:#d8daef;\">PenroseEmperorNewMind</span><sup>[17]</sup>, <span style=\"background-color:#d4efdf;\">MiphamAdornment</span><sup>[7]</sup>, <span style=\"background-color:#d0ece7;\">SearleNeurosciencePhilosophy</span><sup>[24]</sup>, <span style=\"background-color:#ecf3cf;\">SearleMindIntroduction</span><sup>[22]</sup>, <span style=\"background-color:#d6dbdf;\">MiphamVividAwareness</span><sup>[11]</sup>, <span style=\"background-color:#d4e6f1;\">McEvilleyShapeAncientThought</span><sup>[16]</sup>, <span style=\"background-color:#d4e6f1;\">MiphamSpeechDelight</span><sup>[10]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> bye\n",
      "Good bye!\n"
     ]
    }
   ],
   "source": [
    "# Talk to the net!\n",
    "doDialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
